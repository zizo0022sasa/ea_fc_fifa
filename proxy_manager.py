#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒ Ù…Ø¯ÙŠØ± Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ø§Ù„Ø°ÙƒÙŠ - Ù…ØªÙƒØ§Ù…Ù„ Ù…Ø¹ Ø¨ÙˆØª Ø§Ù„ØªÙ„ÙŠØ¬Ø±Ø§Ù…
âœ… Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ù„Ù‰ freefollower.net ÙˆÙ…ÙˆØ§Ù‚Ø¹ Ø¹Ø§Ù„Ù…ÙŠØ©
ğŸ—‘ï¸ Ø­Ø°Ù ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ø§Ù„ÙØ§Ø´Ù„Ø©
"""

import asyncio
import json
import logging
import random
import threading
import time
from dataclasses import asdict, dataclass
from datetime import datetime
from typing import Dict, List, Optional, Set

import aiohttp
import requests

logger = logging.getLogger(__name__)


@dataclass
class ProxyData:
    """Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠ"""

    ip: str
    port: int
    protocol: str = "http"  # http, https, socks4, socks5
    username: Optional[str] = None
    password: Optional[str] = None
    country: Optional[str] = None
    speed_ms: Optional[int] = None
    last_checked: Optional[str] = None
    working_sites: List[str] = None
    success_rate: float = 0.0

    def __post_init__(self):
        if self.working_sites is None:
            self.working_sites = []

    def to_url(self) -> str:
        """ØªØ­ÙˆÙŠÙ„ Ù„ØµÙŠØºØ© URL"""
        if self.username and self.password:
            return f"{self.protocol}://{self.username}:{self.password}@{self.ip}:{self.port}"
        return f"{self.protocol}://{self.ip}:{self.port}"

    def to_dict(self) -> dict:
        """ØªØ­ÙˆÙŠÙ„ Ù„Ù€ dictionary"""
        return asdict(self)

    @classmethod
    def from_string(cls, proxy_str: str) -> "ProxyData":
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù† Ù†Øµ"""
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† ØµÙŠØº Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠ
        if "@" in proxy_str:
            # protocol://user:pass@ip:port
            parts = proxy_str.split("://")
            protocol = parts[0] if len(parts) > 1 else "http"
            rest = parts[1] if len(parts) > 1 else parts[0]

            auth_host = rest.split("@")
            auth = auth_host[0].split(":")
            host_port = auth_host[1].split(":")

            return cls(
                ip=host_port[0],
                port=int(host_port[1]),
                protocol=protocol,
                username=auth[0],
                password=auth[1],
            )
        elif "://" in proxy_str:
            # protocol://ip:port
            parts = proxy_str.split("://")
            protocol = parts[0]
            host_port = parts[1].split(":")
            return cls(ip=host_port[0], port=int(host_port[1]), protocol=protocol)
        else:
            # ip:port
            parts = proxy_str.split(":")
            return cls(ip=parts[0], port=int(parts[1]), protocol="http")


class ProxyManager:
    """Ù…Ø¯ÙŠØ± Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ø§Ù„Ø°ÙƒÙŠ"""

    def __init__(
        self,
        proxy_file: str = "working_proxies.json",
        sources_file: str = "proxy_sources.json",
        auto_clean: bool = True,
    ):
        self.proxy_file = proxy_file
        self.sources_file = sources_file
        self.auto_clean = auto_clean
        self.proxies: List[ProxyData] = []
        self.failed_proxies: Set[str] = set()
        self.lock = threading.Lock()

        # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
        self.test_sites = {
            # Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
            "freefollower": "https://freefollower.net/",
            # Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ©
            "facebook": "https://www.facebook.com/robots.txt",
            "instagram": "https://www.instagram.com/robots.txt",
            "twitter": "https://twitter.com/robots.txt",
            "tiktok": "https://www.tiktok.com/robots.txt",
            "youtube": "https://www.youtube.com/robots.txt",
            "linkedin": "https://www.linkedin.com/robots.txt",
            "pinterest": "https://www.pinterest.com/robots.txt",
            "reddit": "https://www.reddit.com/robots.txt",
            "snapchat": "https://www.snapchat.com/robots.txt",
            "whatsapp": "https://www.whatsapp.com/robots.txt",
            "telegram": "https://telegram.org/robots.txt",
            "discord": "https://discord.com/robots.txt",
            "twitch": "https://www.twitch.tv/robots.txt",
            "tumblr": "https://www.tumblr.com/robots.txt",
            "vk": "https://vk.com/robots.txt",
            "weibo": "https://weibo.com/robots.txt",
            "wechat": "https://www.wechat.com/robots.txt",
            # Ù…Ù†ØµØ§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆØ§Ù„ØªØ±ÙÙŠÙ‡
            "netflix": "https://www.netflix.com/robots.txt",
            "spotify": "https://open.spotify.com/robots.txt",
            "soundcloud": "https://soundcloud.com/robots.txt",
            "vimeo": "https://vimeo.com/robots.txt",
            "dailymotion": "https://www.dailymotion.com/robots.txt",
            "hulu": "https://www.hulu.com/robots.txt",
            "hbo": "https://www.hbo.com/robots.txt",
            "disney": "https://www.disneyplus.com/robots.txt",
            "prime": "https://www.primevideo.com/robots.txt",
            # Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¨Ø­Ø«
            "google": "https://www.google.com/robots.txt",
            "bing": "https://www.bing.com/robots.txt",
            "yahoo": "https://search.yahoo.com/robots.txt",
            "duckduckgo": "https://duckduckgo.com/robots.txt",
            "baidu": "https://www.baidu.com/robots.txt",
            "yandex": "https://yandex.com/robots.txt",
            # Ø§Ù„ØªØ¬Ø§Ø±Ø© Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ©
            "amazon": "https://www.amazon.com/robots.txt",
            "ebay": "https://www.ebay.com/robots.txt",
            "alibaba": "https://www.alibaba.com/robots.txt",
            "aliexpress": "https://www.aliexpress.com/robots.txt",
            "shopify": "https://www.shopify.com/robots.txt",
            "etsy": "https://www.etsy.com/robots.txt",
            # Ù…ÙˆØ§Ù‚Ø¹ ØªÙ‚Ù†ÙŠØ©
            "github": "https://github.com/robots.txt",
            "stackoverflow": "https://stackoverflow.com/robots.txt",
            "gitlab": "https://gitlab.com/robots.txt",
            "bitbucket": "https://bitbucket.org/robots.txt",
            # Ù…ÙˆØ§Ù‚Ø¹ Ø¥Ø®Ø¨Ø§Ø±ÙŠØ©
            "cnn": "https://www.cnn.com/robots.txt",
            "bbc": "https://www.bbc.com/robots.txt",
            "nytimes": "https://www.nytimes.com/robots.txt",
            "theguardian": "https://www.theguardian.com/robots.txt",
            "reuters": "https://www.reuters.com/robots.txt",
            # Ù…Ù†ØµØ§Øª Ø¹Ø±Ø¨ÙŠØ©
            "shahid": "https://shahid.mbc.net/robots.txt",
            "anghami": "https://www.anghami.com/robots.txt",
            "souq": "https://www.souq.com/robots.txt",
            "noon": "https://www.noon.com/robots.txt",
            "careem": "https://www.careem.com/robots.txt",
            # Ù…ÙˆØ§Ù‚Ø¹ Ø¢Ø³ÙŠÙˆÙŠØ©
            "wechat_cn": "https://weixin.qq.com/robots.txt",
            "line": "https://line.me/robots.txt",
            "kakao": "https://www.kakaocorp.com/robots.txt",
            "naver": "https://www.naver.com/robots.txt",
            "rakuten": "https://www.rakuten.com/robots.txt",
            # Ù…ÙˆØ§Ù‚Ø¹ Ø£ÙˆØ±ÙˆØ¨ÙŠØ©
            "spotify_eu": "https://www.spotify.com/de/robots.txt",
            "zalando": "https://www.zalando.com/robots.txt",
            "booking": "https://www.booking.com/robots.txt",
            "skyscanner": "https://www.skyscanner.com/robots.txt",
            # Ù…ÙˆØ§Ù‚Ø¹ Ø£Ù…Ø±ÙŠÙƒØ§ Ø§Ù„Ù„Ø§ØªÙŠÙ†ÙŠØ©
            "mercadolibre": "https://www.mercadolibre.com/robots.txt",
            "globo": "https://www.globo.com/robots.txt",
            # Ù…ÙˆØ§Ù‚Ø¹ Ø£ÙØ±ÙŠÙ‚ÙŠØ©
            "jumia": "https://www.jumia.com/robots.txt",
            "takealot": "https://www.takealot.com/robots.txt",
            # Ø¨Ù†ÙˆÙƒ ÙˆÙ…Ø§Ù„ÙŠØ©
            "paypal": "https://www.paypal.com/robots.txt",
            "stripe": "https://stripe.com/robots.txt",
            "wise": "https://wise.com/robots.txt",
            # Ø£Ù„Ø¹Ø§Ø¨
            "steam": "https://store.steampowered.com/robots.txt",
            "epicgames": "https://www.epicgames.com/robots.txt",
            "roblox": "https://www.roblox.com/robots.txt",
            "minecraft": "https://www.minecraft.net/robots.txt",
            # ØªØ¹Ù„ÙŠÙ…
            "coursera": "https://www.coursera.org/robots.txt",
            "udemy": "https://www.udemy.com/robots.txt",
            "edx": "https://www.edx.org/robots.txt",
            "khanacademy": "https://www.khanacademy.org/robots.txt",
        }

        # Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª
        self.proxy_sources = [
            "https://api.proxyscrape.com/v2/?request=getproxies&protocol=all&timeout=20000&country=all&ssl=all&anonymity=all",
            "https://www.proxy-list.download/api/v1/get?type=http",
            "https://www.proxy-list.download/api/v1/get?type=https",
            "https://www.proxy-list.download/api/v1/get?type=socks4",
            "https://www.proxy-list.download/api/v1/get?type=socks5",
            "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt",
            "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/socks4.txt",
            "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/socks5.txt",
            "https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/http.txt",
            "https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/socks4.txt",
            "https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/socks5.txt",
            "https://raw.githubusercontent.com/hookzof/socks5_list/master/proxy.txt",
            "https://raw.githubusercontent.com/clarketm/proxy-list/master/proxy-list-raw.txt",
            "https://raw.githubusercontent.com/sunny9577/proxy-scraper/master/proxies.txt",
            "https://raw.githubusercontent.com/roosterkid/openproxylist/main/HTTPS_RAW.txt",
            "https://raw.githubusercontent.com/roosterkid/openproxylist/main/SOCKS4_RAW.txt",
            "https://raw.githubusercontent.com/roosterkid/openproxylist/main/SOCKS5_RAW.txt",
            "https://proxylist.geonode.com/api/proxy-list?limit=500&page=1&sort_by=lastChecked&sort_type=desc",
            "https://www.proxyscan.io/download?type=http",
            "https://www.proxyscan.io/download?type=https",
            "https://www.proxyscan.io/download?type=socks4",
            "https://www.proxyscan.io/download?type=socks5",
        ]

        self.load_proxies()

    def load_proxies(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ù…Ù† Ø§Ù„Ù…Ù„Ù"""
        try:
            with open(self.proxy_file, "r", encoding="utf-8") as f:
                data = json.load(f)
                self.proxies = [ProxyData(**p) for p in data]
                logger.info(
                    f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.proxies)} Ø¨Ø±ÙˆÙƒØ³ÙŠ Ù…Ù† {self.proxy_file}"
                )
        except FileNotFoundError:
            logger.warning(f"âš ï¸ Ù…Ù„Ù {self.proxy_file} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
            self.proxies = []
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª: {e}")
            self.proxies = []

    def save_proxies(self):
        """Ø­ÙØ¸ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª ÙÙŠ Ø§Ù„Ù…Ù„Ù"""
        try:
            with self.lock:
                # ÙÙ„ØªØ±Ø© Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ø§Ù„Ø´ØºØ§Ù„Ø© ÙÙ‚Ø·
                working_proxies = [
                    p for p in self.proxies if p.to_url() not in self.failed_proxies
                ]

                data = [p.to_dict() for p in working_proxies]

                with open(self.proxy_file, "w", encoding="utf-8") as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)

                logger.info(
                    f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ {len(working_proxies)} Ø¨Ø±ÙˆÙƒØ³ÙŠ ÙÙŠ {self.proxy_file}"
                )
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª: {e}")

    async def fetch_proxies_from_source(
        self, session: aiohttp.ClientSession, url: str
    ) -> List[str]:
        """Ø¬Ù„Ø¨ Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ù…Ù† Ù…ØµØ¯Ø± ÙˆØ§Ø­Ø¯"""
        try:
            async with session.get(
                url, timeout=aiohttp.ClientTimeout(total=15)
            ) as response:
                if response.status == 200:
                    text = await response.text()

                    # Ù…Ø¹Ø§Ù„Ø¬Ø© JSON Ø¥Ø°Ø§ ÙƒØ§Ù†
                    if url.endswith(".json") or "json" in response.headers.get(
                        "content-type", ""
                    ):
                        try:
                            data = json.loads(text)
                            if isinstance(data, dict) and "data" in data:
                                proxies = []
                                for item in data["data"]:
                                    if "ip" in item and "port" in item:
                                        proxies.append(f"{item['ip']}:{item['port']}")
                                return proxies
                        except:
                            pass

                    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù†Øµ Ø¹Ø§Ø¯ÙŠ
                    lines = text.strip().split("\n")
                    proxies = []
                    for line in lines:
                        line = line.strip()
                        if ":" in line and not line.startswith("#"):
                            proxies.append(line.split()[0] if " " in line else line)

                    return proxies
        except Exception as e:
            logger.debug(f"ÙØ´Ù„ Ø¬Ù„Ø¨ Ù…Ù† {url}: {e}")

        return []

    async def collect_proxies(self) -> List[ProxyData]:
        """Ø¬Ù…Ø¹ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø±"""
        logger.info("ğŸ”„ Ø¬Ù…Ø¹ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø±...")

        all_proxies = set()

        async with aiohttp.ClientSession() as session:
            tasks = [
                self.fetch_proxies_from_source(session, url)
                for url in self.proxy_sources
            ]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            for result in results:
                if isinstance(result, list):
                    all_proxies.update(result)

        # ØªØ­ÙˆÙŠÙ„ Ù„Ù€ ProxyData
        proxy_objects = []
        for proxy_str in all_proxies:
            try:
                proxy = ProxyData.from_string(proxy_str)
                proxy_objects.append(proxy)
            except:
                continue

        logger.info(f"âœ… ØªÙ… Ø¬Ù…Ø¹ {len(proxy_objects)} Ø¨Ø±ÙˆÙƒØ³ÙŠ ÙØ±ÙŠØ¯")
        return proxy_objects

    async def test_proxy(
        self, proxy: ProxyData, test_url: str = None, timeout: int = 20
    ) -> bool:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø±ÙˆÙƒØ³ÙŠ ÙˆØ§Ø­Ø¯"""
        if test_url is None:
            test_url = self.test_sites["freefollower"]

        proxy_url = proxy.to_url()

        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠ Ø­Ø³Ø¨ Ø§Ù„Ù…ÙƒØªØ¨Ø©
        if proxy.protocol in ["socks4", "socks5"]:
            # Ù„Ù„Ù€ SOCKS Ù†Ø­ØªØ§Ø¬ aiohttp-socks
            connector = None  # Ø³Ù†Ø³ØªØ®Ø¯Ù… requests Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† aiohttp Ù„Ù„Ù€ SOCKS

            try:
                proxies = {"http": proxy_url, "https": proxy_url}

                start_time = time.time()
                response = requests.get(
                    test_url,
                    proxies=proxies,
                    timeout=timeout,
                    verify=False,
                    headers={
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                    },
                )

                elapsed = int((time.time() - start_time) * 1000)

                if response.status_code in [200, 301, 302]:
                    proxy.speed_ms = elapsed
                    proxy.last_checked = datetime.now().isoformat()

                    if test_url not in proxy.working_sites:
                        proxy.working_sites.append(test_url)

                    return True

            except Exception as e:
                logger.debug(f"ÙØ´Ù„ {proxy_url}: {e}")
                return False
        else:
            # HTTP/HTTPS
            try:
                connector = aiohttp.TCPConnector(ssl=False)
                timeout_obj = aiohttp.ClientTimeout(total=timeout)

                async with aiohttp.ClientSession(
                    connector=connector, timeout=timeout_obj
                ) as session:
                    start_time = time.time()

                    async with session.get(
                        test_url,
                        proxy=proxy_url,
                        headers={
                            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                        },
                    ) as response:
                        elapsed = int((time.time() - start_time) * 1000)

                        if response.status in [200, 301, 302]:
                            proxy.speed_ms = elapsed
                            proxy.last_checked = datetime.now().isoformat()

                            if test_url not in proxy.working_sites:
                                proxy.working_sites.append(test_url)

                            return True

            except Exception as e:
                logger.debug(f"ÙØ´Ù„ {proxy_url}: {e}")
                return False

        return False

    async def test_all_proxies(
        self, proxies: List[ProxyData] = None, max_workers: int = 50
    ) -> List[ProxyData]:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª"""
        if proxies is None:
            proxies = self.proxies

        if not proxies:
            logger.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±")
            return []

        logger.info(f"ğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø± {len(proxies)} Ø¨Ø±ÙˆÙƒØ³ÙŠ Ø¹Ù„Ù‰ freefollower.net...")

        working_proxies = []

        # Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ø£ÙˆÙ„Ø§Ù‹
        semaphore = asyncio.Semaphore(max_workers)

        async def test_with_semaphore(proxy):
            async with semaphore:
                if await self.test_proxy(proxy, self.test_sites["freefollower"], 20):
                    return proxy
                return None

        tasks = [test_with_semaphore(proxy) for proxy in proxies]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for result in results:
            if result and not isinstance(result, Exception):
                working_proxies.append(result)

        logger.info(f"âœ… {len(working_proxies)} Ø¨Ø±ÙˆÙƒØ³ÙŠ ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ freefollower.net")

        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ø¹Ù„Ù‰ Ù…ÙˆØ§Ù‚Ø¹ Ø£Ø®Ø±Ù‰
        if working_proxies and len(working_proxies) > 0:
            logger.info("ğŸŒ Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ù„Ù‰ Ù…ÙˆØ§Ù‚Ø¹ Ø¹Ø§Ù„Ù…ÙŠØ© Ø¥Ø¶Ø§ÙÙŠØ©...")

            # Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ù„Ù‰ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹
            sample_sites = random.sample(
                list(self.test_sites.items()), min(10, len(self.test_sites))
            )

            for site_name, site_url in sample_sites:
                if site_name == "freefollower":
                    continue

                logger.info(f"ğŸ” Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ù„Ù‰ {site_name}...")

                tasks = [
                    self.test_proxy(proxy, site_url, 10)
                    for proxy in working_proxies[:20]
                ]
                await asyncio.gather(*tasks, return_exceptions=True)

        # Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­
        for proxy in working_proxies:
            if len(proxy.working_sites) > 0:
                proxy.success_rate = (
                    len(proxy.working_sites) / len(self.test_sites) * 100
                )

        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø³Ø±Ø¹Ø©
        working_proxies.sort(key=lambda p: p.speed_ms or 99999)

        return working_proxies

    def mark_proxy_failed(self, proxy_url: str):
        """ØªØ­Ø¯ÙŠØ¯ Ø¨Ø±ÙˆÙƒØ³ÙŠ ÙƒÙØ§Ø´Ù„ ÙˆØ­Ø°ÙÙ‡"""
        with self.lock:
            self.failed_proxies.add(proxy_url)

            # Ø­Ø°Ù Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©
            self.proxies = [p for p in self.proxies if p.to_url() != proxy_url]

            # Ø­ÙØ¸ ØªÙ„Ù‚Ø§Ø¦ÙŠ
            if self.auto_clean:
                self.save_proxies()

            logger.warning(f"ğŸ—‘ï¸ ØªÙ… Ø­Ø°Ù Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠ Ø§Ù„ÙØ§Ø´Ù„: {proxy_url}")

    def get_best_proxy(self) -> Optional[ProxyData]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø¨Ø±ÙˆÙƒØ³ÙŠ Ù…ØªØ§Ø­"""
        with self.lock:
            # ÙÙ„ØªØ±Ø© Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ø§Ù„Ø´ØºØ§Ù„Ø©
            working = [p for p in self.proxies if p.to_url() not in self.failed_proxies]

            if not working:
                return None

            # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆÙ…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­
            working.sort(key=lambda p: (p.speed_ms or 99999, -p.success_rate))

            return working[0]

    def get_random_proxy(self) -> Optional[ProxyData]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¨Ø±ÙˆÙƒØ³ÙŠ Ø¹Ø´ÙˆØ§Ø¦ÙŠ"""
        with self.lock:
            working = [p for p in self.proxies if p.to_url() not in self.failed_proxies]

            if not working:
                return None

            return random.choice(working)

    async def refresh_proxies(self):
        """ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª"""
        logger.info("ğŸ”„ ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª...")

        # Ø¬Ù…Ø¹ Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ø¬Ø¯ÙŠØ¯Ø©
        new_proxies = await self.collect_proxies()

        # Ø§Ø®ØªØ¨Ø§Ø±Ù‡Ø§
        working_proxies = await self.test_all_proxies(new_proxies, max_workers=100)

        if working_proxies:
            with self.lock:
                # Ø¯Ù…Ø¬ Ù…Ø¹ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©
                existing_urls = {p.to_url() for p in self.proxies}

                for proxy in working_proxies:
                    if proxy.to_url() not in existing_urls:
                        self.proxies.append(proxy)

                # Ø­ÙØ¸
                self.save_proxies()

            logger.info(f"âœ… ØªÙ… Ø¥Ø¶Ø§ÙØ© {len(working_proxies)} Ø¨Ø±ÙˆÙƒØ³ÙŠ Ø¬Ø¯ÙŠØ¯")
        else:
            logger.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ø¬Ø¯ÙŠØ¯Ø© Ø´ØºØ§Ù„Ø©")

    def use_proxy(
        self, proxy: ProxyData, url: str, **kwargs
    ) -> Optional[requests.Response]:
        """Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø±ÙˆÙƒØ³ÙŠ Ù…Ø¹ Ø§Ù„Ø­Ø°Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù†Ø¯ Ø§Ù„ÙØ´Ù„"""
        proxy_url = proxy.to_url()

        proxies = {"http": proxy_url, "https": proxy_url}

        try:
            response = requests.get(
                url,
                proxies=proxies,
                timeout=kwargs.get("timeout", 20),
                verify=False,
                headers=kwargs.get(
                    "headers",
                    {
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                    },
                ),
            )

            if response.status_code >= 400:
                # Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠ Ø´ØºØ§Ù„ Ù„ÙƒÙ† Ø±Ø¯ Ø¨Ø®Ø·Ø£
                if response.status_code in [403, 429]:
                    # Ù…Ø­Ø¸ÙˆØ± Ø£Ùˆ rate limited
                    self.mark_proxy_failed(proxy_url)
                    return None

            return response

        except Exception as e:
            # Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠ ÙØ´Ù„
            logger.error(f"âŒ ÙØ´Ù„ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠ {proxy_url}: {e}")
            self.mark_proxy_failed(proxy_url)
            return None

    def get_statistics(self) -> Dict:
        """Ø§Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª"""
        with self.lock:
            total = len(self.proxies)
            failed = len(self.failed_proxies)
            working = total - failed

            speeds = [p.speed_ms for p in self.proxies if p.speed_ms]
            avg_speed = sum(speeds) / len(speeds) if speeds else 0

            protocols = {}
            for p in self.proxies:
                protocols[p.protocol] = protocols.get(p.protocol, 0) + 1

            countries = {}
            for p in self.proxies:
                if p.country:
                    countries[p.country] = countries.get(p.country, 0) + 1

            return {
                "total": total,
                "working": working,
                "failed": failed,
                "avg_speed_ms": avg_speed,
                "protocols": protocols,
                "countries": countries,
                "top_sites": self._get_top_working_sites(),
            }

    def _get_top_working_sites(self) -> List[tuple]:
        """Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø£ÙƒØ«Ø± Ù†Ø¬Ø§Ø­Ø§Ù‹"""
        site_counts = {}

        for proxy in self.proxies:
            for site in proxy.working_sites:
                site_counts[site] = site_counts.get(site, 0) + 1

        return sorted(site_counts.items(), key=lambda x: x[1], reverse=True)[:10]


# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
async def main():
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¯ÙŠØ±
    manager = ProxyManager()

    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª
    await manager.refresh_proxies()

    # Ø§Ø­ØµØ§Ø¦ÙŠØ§Øª
    stats = manager.get_statistics()
    print(f"ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª:")
    print(f"  â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {stats['total']}")
    print(f"  â€¢ Ø´ØºØ§Ù„: {stats['working']}")
    print(f"  â€¢ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø³Ø±Ø¹Ø©: {stats['avg_speed_ms']:.0f}ms")

    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø±ÙˆÙƒØ³ÙŠ
    proxy = manager.get_best_proxy()
    if proxy:
        print(f"ğŸŒ Ø£ÙØ¶Ù„ Ø¨Ø±ÙˆÙƒØ³ÙŠ: {proxy.to_url()}")
        response = manager.use_proxy(proxy, "https://freefollower.net/")
        if response:
            print(f"âœ… Ù†Ø¬Ø­ Ø§Ù„Ø·Ù„Ø¨: {response.status_code}")
        else:
            print("âŒ ÙØ´Ù„ Ø§Ù„Ø·Ù„Ø¨")


if __name__ == "__main__":
    asyncio.run(main())
