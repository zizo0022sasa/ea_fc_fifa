#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒ Ù…Ø¯ÙŠØ± Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ø§Ù„Ù…Ø­Ø³Ù‘Ù† - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„Ø´ØºØ§Ù„Ø©
âœ… Ø¨Ø³Ø§Ø·Ø© ÙˆÙƒÙØ§Ø¡Ø© Ø¹Ø§Ù„ÙŠØ©
ğŸš€ Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹ ÙˆØ¯Ù‚ÙŠÙ‚
"""

import json
import logging
import os
import random
import socket
import threading
import time
import warnings
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from typing import Dict, List, Optional, Set

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ØªØ¹Ø·ÙŠÙ„ ØªØ­Ø°ÙŠØ±Ø§Øª SSL
warnings.filterwarnings("ignore")
requests.packages.urllib3.disable_warnings()

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)


class ProxyTester:
    """ÙØ¦Ø© Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª"""

    def __init__(self):
        self.test_urls = [
            "https://freefollower.net/",
            "https://api.ipify.org/",
            "http://www.google.com/generate_204",
        ]

    def quick_test(self, ip: str, port: int, timeout: int = 3) -> bool:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹ Ù„Ù„Ù…Ù†ÙØ°"""
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(timeout)
            result = sock.connect_ex((ip, port))
            sock.close()
            return result == 0
        except:
            return False

    def test_http_proxy(self, proxy_url: str, timeout: int = 10) -> Dict:
        """Ø§Ø®ØªØ¨Ø§Ø± HTTP proxy"""
        result = {
            "working": False,
            "speed_ms": None,
            "anonymity": "unknown",
            "response_code": None,
        }

        try:
            session = requests.Session()
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ retry strategy
            retry_strategy = Retry(
                total=2,
                backoff_factor=1,
                status_forcelist=[429, 500, 502, 503, 504],
            )
            adapter = HTTPAdapter(max_retries=retry_strategy)
            session.mount("http://", adapter)
            session.mount("https://", adapter)

            proxies = {"http": proxy_url, "https": proxy_url}

            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }

            start_time = time.time()

            # Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ù„Ù‰ Ù…ÙˆÙ‚Ø¹ Ø³Ø±ÙŠØ¹
            response = session.get(
                "http://httpbin.org/ip",
                proxies=proxies,
                headers=headers,
                timeout=timeout,
                verify=False,
            )

            elapsed_ms = int((time.time() - start_time) * 1000)

            if response.status_code == 200:
                result["working"] = True
                result["speed_ms"] = elapsed_ms
                result["response_code"] = response.status_code

                # ÙØ­Øµ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥Ø®ÙØ§Ø¡
                try:
                    response_data = response.json()
                    origin_ip = response_data.get("origin", "").split(",")[0].strip()
                    proxy_ip = proxy_url.split("@")[-1].split(":")[0]

                    if proxy_ip not in origin_ip:
                        result["anonymity"] = "elite"
                    else:
                        result["anonymity"] = "transparent"
                except:
                    result["anonymity"] = "anonymous"

        except requests.exceptions.ConnectTimeout:
            pass
        except requests.exceptions.ProxyError:
            pass
        except requests.exceptions.ConnectionError:
            pass
        except Exception:
            pass

        return result


class ProxyManager:
    """Ù…Ø¯ÙŠØ± Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…Ø¨Ø³Ø·"""

    def __init__(self):
        self.working_proxies = []
        self.failed_proxies = set()
        self.lock = threading.Lock()
        self.tester = ProxyTester()
        
        # Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ†
        self.working_file = "working_proxies.json"
        self.failed_file = "failed_proxies.json"

        # Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª - ØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£ÙØ¶Ù„ ÙÙ‚Ø·
        self.sources = [
            "https://api.proxyscrape.com/v2/?request=displayproxies&protocol=http&timeout=10000&country=all&ssl=all&anonymity=all&simplified=true",
            "https://www.proxy-list.download/api/v1/get?type=http",
            "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt",
            "https://raw.githubusercontent.com/ShiftyTR/Proxy-List/master/http.txt",
            "https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/http.txt",
            "https://raw.githubusercontent.com/clarketm/proxy-list/master/proxy-list-raw.txt",
            "https://raw.githubusercontent.com/sunny9577/proxy-scraper/master/proxies.txt",
            "https://raw.githubusercontent.com/roosterkid/openproxylist/main/HTTPS_RAW.txt",
            "https://raw.githubusercontent.com/Zaeem20/FREE_PROXIES_LIST/master/http.txt",
            "https://raw.githubusercontent.com/prxchk/proxy-list/main/http.txt",
        ]

        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©
        self.load_data()

    def load_data(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©"""
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ø§Ù„Ø´ØºØ§Ù„Ø©
        try:
            if os.path.exists(self.working_file):
                with open(self.working_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    self.working_proxies = data
                    logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.working_proxies)} Ø¨Ø±ÙˆÙƒØ³ÙŠ Ø´ØºØ§Ù„")
        except:
            self.working_proxies = []

        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ø§Ù„ÙØ§Ø´Ù„Ø©
        try:
            if os.path.exists(self.failed_file):
                with open(self.failed_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    self.failed_proxies = set(data.get("failed", []))
                    logger.info(f"ğŸ“‹ ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.failed_proxies)} Ø¨Ø±ÙˆÙƒØ³ÙŠ ÙØ§Ø´Ù„")
        except:
            self.failed_proxies = set()

    def save_data(self):
        """Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            # Ø­ÙØ¸ Ø§Ù„Ø´ØºØ§Ù„Ø©
            with open(self.working_file, "w", encoding="utf-8") as f:
                json.dump(self.working_proxies, f, ensure_ascii=False, indent=2)

            # Ø­ÙØ¸ Ø§Ù„ÙØ§Ø´Ù„Ø©
            failed_data = {
                "failed": list(self.failed_proxies)[:5000],  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 5000
                "last_updated": datetime.now().isoformat(),
            }
            with open(self.failed_file, "w", encoding="utf-8") as f:
                json.dump(failed_data, f, ensure_ascii=False, indent=2)

            logger.info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ {len(self.working_proxies)} Ø¨Ø±ÙˆÙƒØ³ÙŠ Ø´ØºØ§Ù„")
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ÙØ¸: {e}")

    def fetch_from_source(self, url: str) -> List[str]:
        """Ø¬Ù„Ø¨ Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ù…Ù† Ù…ØµØ¯Ø± ÙˆØ§Ø­Ø¯"""
        proxies = []
        
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
            
            response = requests.get(url, headers=headers, timeout=15, verify=False)
            
            if response.status_code == 200:
                content = response.text
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø³ÙŠØ· ÙˆÙØ¹Ø§Ù„
                lines = content.split('\n')
                
                for line in lines:
                    line = line.strip()
                    
                    # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ÙØ§Ø±ØºØ© ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª
                    if not line or line.startswith('#') or line.startswith('//'):
                        continue
                    
                    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø³Ø·Ø±
                    line = line.replace('http://', '').replace('https://', '')
                    
                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù†Ù…Ø· IP:Port
                    if ':' in line:
                        parts = line.split()
                        proxy_part = parts[0]  # Ø£ÙˆÙ„ Ø¬Ø²Ø¡ Ø¹Ø§Ø¯Ø© ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ IP:Port
                        
                        try:
                            if '@' in proxy_part:
                                # Ø¥Ø²Ø§Ù„Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØµØ§Ø¯Ù‚Ø© Ø¥Ù† ÙˆØ¬Ø¯Øª
                                proxy_part = proxy_part.split('@')[-1]
                            
                            ip, port = proxy_part.rsplit(':', 1)
                            port_num = int(port)
                            
                            # ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…Ù†ÙØ°
                            if 1 <= port_num <= 65535:
                                proxies.append(f"{ip}:{port}")
                                
                        except (ValueError, IndexError):
                            continue
                
                logger.info(f"âœ“ Ø¬Ù„Ø¨ {len(proxies)} Ù…Ù† {url.split('/')[2]}")
                
        except Exception as e:
            logger.debug(f"âœ— ÙØ´Ù„ {url.split('/')[2] if '/' in url else url}: {type(e).__name__}")
        
        return proxies

    def collect_proxies(self) -> List[str]:
        """Ø¬Ù…Ø¹ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø±"""
        logger.info("ğŸ”„ Ø¬Ù…Ø¹ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø±...")
        
        all_proxies = set()
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… threading Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
        with ThreadPoolExecutor(max_workers=10) as executor:
            results = list(executor.map(self.fetch_from_source, self.sources))
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        for proxy_list in results:
            for proxy in proxy_list:
                # ØªØ¬Ù†Ø¨ Ø§Ù„ÙØ§Ø´Ù„Ø© ÙˆØ§Ù„Ù…ÙƒØ±Ø±Ø©
                if proxy not in self.failed_proxies:
                    all_proxies.add(proxy)
        
        proxy_list = list(all_proxies)
        random.shuffle(proxy_list)  # Ø®Ù„Ø· Ø¹Ø´ÙˆØ§Ø¦ÙŠ
        
        logger.info(f"âœ… ØªÙ… Ø¬Ù…Ø¹ {len(proxy_list)} Ø¨Ø±ÙˆÙƒØ³ÙŠ ÙØ±ÙŠØ¯")
        return proxy_list

    def test_proxy(self, proxy_str: str) -> Optional[Dict]:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø±ÙˆÙƒØ³ÙŠ ÙˆØ§Ø­Ø¯"""
        try:
            ip, port = proxy_str.split(':')
            port = int(port)
            
            # Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹ Ø£ÙˆÙ„Ø§Ù‹
            if not self.tester.quick_test(ip, port, timeout=3):
                return None
            
            # Ø§Ø®ØªØ¨Ø§Ø± HTTP
            proxy_url = f"http://{proxy_str}"
            result = self.tester.test_http_proxy(proxy_url, timeout=10)
            
            if result["working"]:
                return {
                    "proxy": proxy_str,
                    "ip": ip,
                    "port": port,
                    "protocol": "http",
                    "speed_ms": result["speed_ms"],
                    "anonymity": result["anonymity"],
                    "last_checked": datetime.now().isoformat(),
                    "success_count": 1,
                    "fail_count": 0,
                }
            
        except Exception:
            pass
        
        return None

    def test_all_proxies(self, proxy_list: List[str], max_workers: int = 100) -> List[Dict]:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª"""
        if not proxy_list:
            logger.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±")
            return []

        logger.info(f"ğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø± {len(proxy_list)} Ø¨Ø±ÙˆÙƒØ³ÙŠ...")
        
        working_proxies = []
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª Ù„ØªØ¬Ù†Ø¨ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            batch_size = 500
            total_tested = 0
            
            for i in range(0, len(proxy_list), batch_size):
                batch = proxy_list[i:i + batch_size]
                
                # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¯ÙØ¹Ø©
                results = list(executor.map(self.test_proxy, batch))
                
                # Ø¬Ù…Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø´ØºØ§Ù„Ø©
                batch_working = [r for r in results if r is not None]
                working_proxies.extend(batch_working)
                
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙØ§Ø´Ù„Ø©
                failed_batch = [
                    proxy for proxy, result in zip(batch, results) 
                    if result is None
                ]
                self.failed_proxies.update(failed_batch)
                
                total_tested += len(batch)
                success_rate = (len(working_proxies) / total_tested * 100) if total_tested > 0 else 0
                
                logger.info(
                    f"ğŸ“Š Ø§Ù„ØªÙ‚Ø¯Ù…: {total_tested}/{len(proxy_list)} | "
                    f"âœ… Ø´ØºØ§Ù„: {len(working_proxies)} | "
                    f"ğŸ“ˆ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {success_rate:.1f}%"
                )
                
                # Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù†Ø§ Ø¹Ø¯Ø¯ ÙƒØ§ÙÙŠØŒ Ù†ÙƒØªÙÙŠ
                if len(working_proxies) >= 200:
                    logger.info("âœ¨ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ ÙƒØ§ÙÙŠ Ù…Ù† Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª")
                    break
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø³Ø±Ø¹Ø©
        working_proxies.sort(key=lambda p: p.get('speed_ms', 99999))
        
        logger.info(f"ğŸ‰ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: {len(working_proxies)} Ø¨Ø±ÙˆÙƒØ³ÙŠ Ø´ØºØ§Ù„!")
        return working_proxies

    def refresh_proxies(self):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª"""
        logger.info("=" * 60)
        logger.info("ğŸš€ Ø¨Ø¯Ø¡ ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª...")
        logger.info("=" * 60)
        
        # Ø¬Ù…Ø¹ Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ø¬Ø¯ÙŠØ¯Ø©
        new_proxies = self.collect_proxies()
        
        if new_proxies:
            # Ø§Ø®ØªØ¨Ø§Ø±
            working_new = self.test_all_proxies(new_proxies[:2000])  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 2000 Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
            
            if working_new:
                with self.lock:
                    # Ø¯Ù…Ø¬ Ù…Ø¹ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©
                    existing_ips = {p.get('ip') for p in self.working_proxies}
                    
                    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙÙ‚Ø·
                    for proxy in working_new:
                        if proxy['ip'] not in existing_ips:
                            self.working_proxies.append(proxy)
                    
                    # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø³Ø±Ø¹Ø©
                    self.working_proxies.sort(key=lambda p: p.get('speed_ms', 99999))
                    
                    # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø£ÙØ¶Ù„ 500 ÙÙ‚Ø·
                    self.working_proxies = self.working_proxies[:500]
                
                # Ø­ÙØ¸
                self.save_data()
                
                logger.info("=" * 60)
                logger.info(f"âœ… Ø§Ù„Ù†Ø¬Ø§Ø­! Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ø§Ù„Ø´ØºØ§Ù„Ø©: {len(self.working_proxies)}")
                logger.info("=" * 60)
            else:
                logger.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ø´ØºØ§Ù„Ø©")
        else:
            logger.error("âŒ ÙØ´Ù„ Ø¬Ù…Ø¹ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø±")

    def get_best_proxy(self) -> Optional[Dict]:
        """Ø£ÙØ¶Ù„ Ø¨Ø±ÙˆÙƒØ³ÙŠ"""
        with self.lock:
            if self.working_proxies:
                return self.working_proxies[0]  # Ø£Ø³Ø±Ø¹ Ø¨Ø±ÙˆÙƒØ³ÙŠ
        return None

    def get_random_proxy(self) -> Optional[Dict]:
        """Ø¨Ø±ÙˆÙƒØ³ÙŠ Ø¹Ø´ÙˆØ§Ø¦ÙŠ"""
        with self.lock:
            if self.working_proxies:
                return random.choice(self.working_proxies)
        return None

    def test_proxy_on_site(self, proxy_dict: Dict, url: str = "https://httpbin.org/ip") -> bool:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø±ÙˆÙƒØ³ÙŠ Ø¹Ù„Ù‰ Ù…ÙˆÙ‚Ø¹ Ù…Ø­Ø¯Ø¯"""
        try:
            proxy_url = f"http://{proxy_dict['proxy']}"
            proxies = {"http": proxy_url, "https": proxy_url}
            
            response = requests.get(
                url,
                proxies=proxies,
                timeout=15,
                verify=False,
                headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
            )
            
            return response.status_code == 200
            
        except:
            return False

    def get_statistics(self) -> Dict:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…ÙØµÙ„Ø©"""
        with self.lock:
            if not self.working_proxies:
                return {"error": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª"}
            
            speeds = [p.get('speed_ms', 0) for p in self.working_proxies if p.get('speed_ms')]
            avg_speed = sum(speeds) / len(speeds) if speeds else 0
            
            anonymity_levels = {}
            for p in self.working_proxies:
                level = p.get('anonymity', 'unknown')
                anonymity_levels[level] = anonymity_levels.get(level, 0) + 1
            
            return {
                "total_working": len(self.working_proxies),
                "total_failed": len(self.failed_proxies),
                "avg_speed_ms": round(avg_speed, 2),
                "fastest_speed": min(speeds) if speeds else 0,
                "slowest_speed": max(speeds) if speeds else 0,
                "anonymity_levels": anonymity_levels,
                "top_10": [
                    {
                        "proxy": p['proxy'],
                        "speed_ms": p.get('speed_ms', 'N/A'),
                        "anonymity": p.get('anonymity', 'unknown')
                    }
                    for p in self.working_proxies[:10]
                ]
            }


def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    
    print("\n" + "=" * 60)
    print("ğŸš€ Ù…Ø¯ÙŠØ± Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª Ø§Ù„Ù…Ø­Ø³Ù‘Ù† - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©")
    print("=" * 60)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¯ÙŠØ±
    manager = ProxyManager()
    
    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª
    manager.refresh_proxies()
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    stats = manager.get_statistics()
    
    if "error" not in stats:
        print(f"\nğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
        print(f"â”œâ”€ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø´ØºØ§Ù„: {stats['total_working']}")
        print(f"â”œâ”€ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙØ§Ø´Ù„: {stats['total_failed']}")
        print(f"â”œâ”€ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø³Ø±Ø¹Ø©: {stats['avg_speed_ms']}ms")
        print(f"â”œâ”€ Ø£Ø³Ø±Ø¹ Ø¨Ø±ÙˆÙƒØ³ÙŠ: {stats['fastest_speed']}ms")
        print(f"â””â”€ Ø£Ø¨Ø·Ø£ Ø¨Ø±ÙˆÙƒØ³ÙŠ: {stats['slowest_speed']}ms")
        
        if stats['anonymity_levels']:
            print(f"\nğŸ”’ Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¥Ø®ÙØ§Ø¡:")
            for level, count in stats['anonymity_levels'].items():
                print(f"â”œâ”€ {level}: {count}")
        
        print(f"\nğŸ† Ø£ÙØ¶Ù„ 5 Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª:")
        for i, p in enumerate(stats['top_10'][:5], 1):
            print(f"{i}. {p['proxy']} - {p['speed_ms']}ms - {p['anonymity']}")
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø£ÙØ¶Ù„ Ø¨Ø±ÙˆÙƒØ³ÙŠ
        print(f"\nğŸ”§ Ø§Ø®ØªØ¨Ø§Ø± Ø£ÙØ¶Ù„ Ø¨Ø±ÙˆÙƒØ³ÙŠ...")
        best_proxy = manager.get_best_proxy()
        
        if best_proxy:
            print(f"Ø§Ø³ØªØ®Ø¯Ø§Ù…: {best_proxy['proxy']}")
            
            # Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ù„Ù‰ httpbin
            if manager.test_proxy_on_site(best_proxy):
                print(f"âœ… Ù†Ø¬Ø­ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±!")
                
                # Ø§Ø®ØªØ¨Ø§Ø± ÙØ¹Ù„ÙŠ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ IP
                try:
                    proxy_url = f"http://{best_proxy['proxy']}"
                    response = requests.get(
                        "http://httpbin.org/ip",
                        proxies={"http": proxy_url, "https": proxy_url},
                        timeout=10
                    )
                    if response.status_code == 200:
                        ip_data = response.json()
                        print(f"ğŸŒ Ø§Ù„Ù€ IP Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {ip_data.get('origin', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}")
                except:
                    pass
            else:
                print(f"âŒ ÙØ´Ù„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±")
        
    else:
        print(f"âŒ {stats['error']}")
    
    print(f"\nâœ¨ Ø§ÙƒØªÙ…Ù„!")
    print(f"ğŸ’¾ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ: working_proxies.json")
    
    return manager


if __name__ == "__main__":
    try:
        manager = main()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Ø¥ÙŠÙ‚Ø§Ù...")
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø£: {e}")
        import traceback
        traceback.print_exc()
